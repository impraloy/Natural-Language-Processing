{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4989fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training file\n",
    "train_corpus = []\n",
    "with open('train.txt',encoding=\"utf8\") as readFile:\n",
    "    for text in readFile:\n",
    "        train_corpus.append(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05f490db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the testing file\n",
    "test_corpus = []\n",
    "with open('test.txt',encoding=\"utf8\") as readFile:\n",
    "    for text in readFile:\n",
    "        test_corpus.append(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d22666ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unigram Model\n",
    "unigram_dist = {}\n",
    "for text in train_corpus:\n",
    "    text = text.lower() + ' </s>'\n",
    "    for word in text.split():\n",
    "        if word in unigram_dist:\n",
    "            unigram_dist[word] += 1\n",
    "        else:\n",
    "            unigram_dist[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34f0369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Types(Unique words) in training Corpus: 41738\n"
     ]
    }
   ],
   "source": [
    "# 1.3  Q 1:\n",
    "count = 0\n",
    "unk_flag = False\n",
    "for word in unigram_dist:\n",
    "    if unigram_dist[word] == 1:\n",
    "        unk_flag = True\n",
    "    else:\n",
    "        count += 1\n",
    "if unk_flag:\n",
    "    count += 1\n",
    "print('Word Types(Unique words) in training Corpus: {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad742f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens in training Corpus: 2468210\n"
     ]
    }
   ],
   "source": [
    "# 1.3  Q 2:\n",
    "count = 0\n",
    "for word in unigram_dist:\n",
    "    count += unigram_dist[word]\n",
    "print('Word Tokens in training Corpus: {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2803986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No I did not mapped the unknown words to <unk> in training and testing data\n",
      "Word types percentage did not occur in training: 3.605769230769229 %\n",
      "Word tokens percentage did not occur in training: 1.6612495485734957 %\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Q 3\n",
    "words_in_training = []\n",
    "words_not_in_training = []\n",
    "for text in test_corpus:\n",
    "    text = text.lower() + ' </s>'\n",
    "    for word in text.split():\n",
    "        if word in unigram_dist:\n",
    "            words_in_training.append(word)\n",
    "        else:\n",
    "            words_not_in_training.append(word)\n",
    "print('No I did not mapped the unknown words to <unk> in training and testing data')\n",
    "print('Word types percentage did not occur in training: {} %'.format((1-(len(set(words_in_training))/(len(set(words_in_training))+len(set(words_not_in_training)))))*100))\n",
    "print('Word tokens percentage did not occur in training: {} %'.format((1-(len(words_in_training)/(len(words_in_training)+len(words_not_in_training))))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e727ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the singletons into <unk>\n",
    "update_unigram_dist = {}\n",
    "for word in unigram_dist:\n",
    "    if unigram_dist[word] == 1:\n",
    "        if '<unk>' not in update_unigram_dist:\n",
    "            update_unigram_dist['<unk>'] = 0\n",
    "        else:\n",
    "            update_unigram_dist['<unk>'] += 1\n",
    "    else:\n",
    "        update_unigram_dist[word] = unigram_dist[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e220d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigram_dist = {}\n",
    "for text in train_corpus:\n",
    "    text = text.lower() + ' </s>'\n",
    "    tokens = text.split()\n",
    "    for i in range(1,len(tokens)):\n",
    "        w0 = tokens[i-1]\n",
    "        w1 = tokens[i]\n",
    "        if w0 not in update_unigram_dist:\n",
    "            w0 = '<unk>'\n",
    "        if w1 not in update_unigram_dist:\n",
    "            w1 = '<unk>'\n",
    "        if (w0,w1) in bigram_dist:\n",
    "            bigram_dist[(w0,w1)] += 1\n",
    "        else:\n",
    "            bigram_dist[(w0,w1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "173b9d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams types percentage did not occur in training: 25.869565217391298 %\n",
      "Bigrams tokens percentage did not occur in training: 22.36792806294492 %\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Q 4\n",
    "bigrams_in_training = []\n",
    "bigrams_not_in_training = []\n",
    "for text in test_corpus:\n",
    "    text = text.lower() + ' </s>'\n",
    "    tokens = text.split()\n",
    "    for i in range(1,len(tokens)):\n",
    "        w0 = tokens[i-1]\n",
    "        w1 = tokens[i]\n",
    "        if w0 not in update_unigram_dist:\n",
    "            w0 = '<unk>'\n",
    "        if w1 not in update_unigram_dist:\n",
    "            w1 = '<unk>'\n",
    "        if (w0,w1) in bigram_dist:\n",
    "            bigrams_in_training.append((w0,w1))\n",
    "        else:\n",
    "            bigrams_not_in_training.append((w0,w1))\n",
    "print('Bigrams types percentage did not occur in training: {} %'.format((1-(len(set(bigrams_in_training))/(len(set(bigrams_in_training))+len(set(bigrams_not_in_training)))))*100))\n",
    "print('Bigrams tokens percentage did not occur in training: {} %'.format((1-(len(bigrams_in_training)/(len(bigrams_in_training)+len(bigrams_not_in_training))))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ab52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramN = 0\n",
    "for word in update_unigram_dist:\n",
    "    unigramN += update_unigram_dist[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2146ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 0.0029734110847177042)\n",
      "('look', 0.0002483582225006067)\n",
      "('forward', 0.00019204208395642345)\n",
      "('to', 0.02149250732008513)\n",
      "('hearing', 8.467678385420359e-05)\n",
      "('your', 0.0004930700763184965)\n",
      "('reply', 5.266976986146635e-06)\n",
      "('</s>', 0.04051520758574335)\n",
      "\n",
      "Log probability: -84.9288472425838\n"
     ]
    }
   ],
   "source": [
    "# 1.3  Q 5:\n",
    "import numpy as np\n",
    "def unigramLogProb(text):\n",
    "    tokens = (text.lower()+' </s>').split()\n",
    "    log_value = 0\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        if token not in update_unigram_dist:\n",
    "            token = '<unk>'\n",
    "        prob = update_unigram_dist[token]/unigramN\n",
    "        value = np.log2(prob)\n",
    "        token_list.append((token,prob))\n",
    "        log_value += value\n",
    "    return log_value, token_list\n",
    "text = \"I look forward to hearing your reply\"\n",
    "log_prob, token_list = unigramLogProb(text)\n",
    "for token in token_list:\n",
    "    print(token)\n",
    "print('\\nLog probability: {}'.format(log_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "020f1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('i', 'look'), 0.0020438751873552256)\n",
      "(('look', 'forward'), 0.05546492659053834)\n",
      "(('forward', 'to'), 0.2109704641350211)\n",
      "(('to', 'hearing'), 0.00011310511235107827)\n",
      "(('hearing', 'your'), 0.0)\n",
      "(('your', 'reply'), 0.0)\n",
      "(('reply', '</s>'), 0.0)\n",
      "\n",
      "Log probability: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raza/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "def bigramLogProb(text):\n",
    "    tokens = (text.lower()+' </s>').split()\n",
    "    log_value = 0\n",
    "    token_list = []\n",
    "    for i in range(1,len(tokens)):\n",
    "        w0 = tokens[i-1]\n",
    "        w1 = tokens[i]\n",
    "        if w0 not in update_unigram_dist:\n",
    "            w0 = '<unk>'\n",
    "        if w1 not in update_unigram_dist:\n",
    "            w1 = '<unk>'\n",
    "        if (w0,w1) not in bigram_dist:\n",
    "            bigram_occur = 0\n",
    "        else:\n",
    "            bigram_occur = bigram_dist[(w0,w1)]\n",
    "        prob = bigram_occur/update_unigram_dist[w0]\n",
    "        value = np.log2(prob)\n",
    "        token_list.append(((tokens[i-1],tokens[i]),prob))\n",
    "        log_value += value\n",
    "    return log_value, token_list\n",
    "text = \"I look forward to hearing your reply\"\n",
    "log_prob, token_list = bigramLogProb(text)\n",
    "for token in token_list:\n",
    "    print(token)\n",
    "print('\\nLog probability: {}'.format(log_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d0725ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('i', 'look'), 0.0003260182977769627)\n",
      "(('look', 'forward'), 0.0008264267667823664)\n",
      "(('forward', 'to'), 0.0023926845446792383)\n",
      "(('to', 'hearing'), 7.38505686493786e-05)\n",
      "(('hearing', 'your'), 2.3839607123274607e-05)\n",
      "(('your', 'reply'), 2.3280176929344664e-05)\n",
      "(('reply', '</s>'), 2.3951522119230678e-05)\n",
      "\n",
      "Log probability: -90.35211196443383\n"
     ]
    }
   ],
   "source": [
    "def bigramLogProbAddSmooth(text):\n",
    "    tokens = (text.lower()+' </s>').split()\n",
    "    log_value = 0\n",
    "    token_list = []\n",
    "    for i in range(1,len(tokens)):\n",
    "        w0 = tokens[i-1]\n",
    "        w1 = tokens[i]\n",
    "        if w0 not in update_unigram_dist:\n",
    "            w0 = '<unk>'\n",
    "        if w1 not in update_unigram_dist:\n",
    "            w1 = '<unk>'\n",
    "        if (w0,w1) not in bigram_dist:\n",
    "            bigram_occur = 0\n",
    "        else:\n",
    "            bigram_occur = bigram_dist[(w0,w1)]\n",
    "        prob = (bigram_occur+1)/(update_unigram_dist[w0]+len(update_unigram_dist))\n",
    "        value = np.log2(prob)\n",
    "        token_list.append(((tokens[i-1],tokens[i]),prob))\n",
    "        log_value += value\n",
    "    return log_value, token_list\n",
    "text = \"I look forward to hearing your reply\"\n",
    "log_prob, token_list = bigramLogProbAddSmooth(text)\n",
    "for token in token_list:\n",
    "    print(token)\n",
    "print('\\nLog probability: {}'.format(log_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1155561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigramPerplexity(corpus):\n",
    "    sum = 0\n",
    "    N = 0\n",
    "    for text in corpus:\n",
    "        words = text.split()\n",
    "        N = N + len(words)+1 # +1 </s> tags\n",
    "        sum = sum + unigramLogProb(text)[0]\n",
    "    pp = np.power(2, -(sum/N))\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e47d6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramPerplexity(corpus):\n",
    "    sum = 0\n",
    "    N = 0\n",
    "    for text in corpus:\n",
    "        words = text.split()\n",
    "        N = N + len(words)+1 # +1 </s> tags\n",
    "        sum = sum + bigramLogProb(text)[0]\n",
    "    pp = np.power(2, -(sum/N))\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c48e9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramPerplexityAddSmooth(corpus):\n",
    "    sum = 0\n",
    "    N = 0\n",
    "    for text in corpus:\n",
    "        words = text.split()\n",
    "        N = N + len(words)+1 # +1 </s> tags\n",
    "        sum = sum + bigramLogProbAddSmooth(text)[0]\n",
    "    pp = np.power(2, -(sum/N))\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f34f0749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569.5180501674179\n",
      "inf\n",
      "2510.9437518832056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raza/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "# 1.3  Q 6:\n",
    "text = \"I look forward to hearing your reply\"\n",
    "print(unigramPerplexity([text]))\n",
    "print(bigramPerplexity([text]))\n",
    "print(bigramPerplexityAddSmooth([text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55c94a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097.1906220380547\n",
      "inf\n",
      "1905.9287857229451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raza/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "# 1.3  Q 7:\n",
    "print(unigramPerplexity(test_corpus))\n",
    "print(bigramPerplexity(test_corpus))\n",
    "print(bigramPerplexityAddSmooth(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603d020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
